# Outil de Scraping Shopify

Outil automatis√© en Python pour identifier des sites web utilisant Shopify, scraper leurs pages de contact pour extraire les adresses e-mail et num√©ros de t√©l√©phone, puis stocker ces informations dans une base de donn√©es PostgreSQL.

## üìã Structure du Projet

Le projet est organis√© en deux phases distinctes :

- **Phase 1** : D√©couverte de sites Shopify (m√©thode gratuite ou via API BuiltWith)
- **Phase 2** : Scraping des pages de contact et stockage en base de donn√©es

## üöÄ Installation

### Pr√©requis

- Python 3.8 ou sup√©rieur
- PostgreSQL 12 ou sup√©rieur
- Cl√© API BuiltWith (obtenez-la sur [https://builtwith.com/api](https://builtwith.com/api))

### ‚ö†Ô∏è Important : Choix de l'API BuiltWith

BuiltWith propose plusieurs APIs selon votre plan :

**1. Catalog API** (Recommand√© - Plans payants)
- Acc√®s direct au **catalogue BuiltWith** des sites Shopify
- Id√©al pour la Phase 1 du projet - m√©thode la plus efficace
- N√©cessite un plan payant avec acc√®s √† cette fonctionnalit√©
- Configurez `BUILTWITH_API_METHOD=catalog` dans `.env`

**2. Technology Search API** (Plans payants)
- Permet de rechercher des sites par technologie avec filtres
- Alternative au catalogue si vous avez besoin de filtres sp√©cifiques
- Configurez `BUILTWITH_API_METHOD=technology_search` dans `.env`

**2. Domain Lookup API** (Gratuit avec limites ou Payant)
- Permet d'**analyser** un domaine sp√©cifique pour v√©rifier les technologies
- Ne permet PAS de d√©couvrir des sites par technologie
- Utile si vous avez d√©j√† une liste de domaines √† v√©rifier
- Configurez `BUILTWITH_API_METHOD=domain_lookup` dans `.env`
- N√©cessite un fichier `domains_to_check.txt` avec les domaines √† v√©rifier

**3. Free API** (Gratuit mais tr√®s limit√©)
- Similaire √† Domain Lookup mais avec des limites strictes
- Utile uniquement pour des tests

**Recommandation** : Si vous voulez d√©couvrir des sites Shopify automatiquement, vous avez besoin d'un plan BuiltWith avec acc√®s √† la **Technology Search API**. Sinon, vous devrez g√©n√©rer une liste de domaines par d'autres moyens (recherche Google, listes publiques, etc.) et utiliser Domain Lookup pour les v√©rifier.

### √âtapes d'installation

1. **Cloner ou t√©l√©charger le projet**

2. **Installer les d√©pendances Python**

```bash
pip install -r requirements.txt
```

3. **Configurer les variables d'environnement**

Copiez le fichier `env.example` vers `.env` et remplissez les valeurs :

```bash
cp env.example .env
```

√âditez le fichier `.env` avec vos informations :

```env
BUILTWITH_API_KEY=votre_cle_api_builtwith
BUILTWITH_API_METHOD=catalog
MAX_RESULTS=1000
DB_HOST=localhost
DB_PORT=5432
DB_NAME=shopify_scraper
DB_USER=postgres
DB_PASSWORD=votre_mot_de_passe
```

4. **Cr√©er la base de donn√©es PostgreSQL**

Connectez-vous √† PostgreSQL et cr√©ez la base de donn√©es :

```sql
CREATE DATABASE shopify_scraper;
```

5. **Cr√©er les tables**

Ex√©cutez le script SQL pour cr√©er la structure de la base de donn√©es :

```bash
psql -U postgres -d shopify_scraper -f schema.sql
```

Ou depuis psql :

```sql
\c shopify_scraper
\i schema.sql
```

## üìñ Utilisation

### Phase 1 : D√©couverte de sites Shopify

Vous avez **deux options** pour d√©couvrir des sites Shopify :

#### Option A : M√©thode GRATUITE (Recommand√©e) üÜì

Le script `discover_free.py` utilise plusieurs m√©thodes gratuites pour d√©couvrir des sites Shopify **sans aucune cl√© API payante**.

**‚ö†Ô∏è Important : Utilisation de Selenium (recommand√©)**

Par d√©faut, le script utilise **Selenium** avec un navigateur r√©el (Chrome, Firefox ou Edge) pour contourner les blocages des moteurs de recherche comme Bing. Cela permet d'√™tre beaucoup moins d√©tectable qu'avec de simples requ√™tes HTTP.

**Configuration Selenium dans `.env` :**

```env
# Activer Selenium (recommand√© pour √©viter les blocages)
USE_SELENIUM=true

# Navigateur √† utiliser: chrome, firefox, edge
# Chrome est recommand√© pour la meilleure compatibilit√©
SELENIUM_BROWSER=chrome

# Mode headless (sans interface graphique) - true ou false
SELENIUM_HEADLESS=true
```

**Installation des drivers Selenium :**

Les drivers sont automatiquement t√©l√©charg√©s et g√©r√©s par `webdriver-manager`. Assurez-vous d'avoir install√© les d√©pendances :

```bash
pip install -r requirements.txt
```

**Note :** Si Selenium n'est pas disponible ou √©choue, le script basculera automatiquement sur `requests` (mais peut √™tre bloqu√© par Bing).

Le script `discover_free.py` utilise plusieurs m√©thodes gratuites pour d√©couvrir des sites Shopify :

- **Recherche DuckDuckGo** : Plus permissif que Google, permet de nombreuses requ√™tes
- **Scraping de shop.app** : Annuaire public de sites Shopify
- **G√©n√©ration de patterns** : Teste des combinaisons possibles de noms de stores
- **Listes publiques** : Extrait depuis GitHub, forums, etc.

**Utilisation :**
```bash
python discover_free.py
```

**Configuration dans `.env` :**
```env
MAX_RESULTS=5000  # Nombre maximum de domaines √† d√©couvrir
DELAY_BETWEEN_REQUESTS=1.0  # D√©lai entre requ√™tes (secondes)
```

**Avantages :**
- ‚úÖ 100% gratuit
- ‚úÖ Aucune cl√© API requise
- ‚úÖ Plusieurs m√©thodes combin√©es pour maximiser les r√©sultats
- ‚úÖ Contr√¥le total sur le nombre de requ√™tes

**Inconv√©nients :**
- ‚ö†Ô∏è Plus lent que l'API BuiltWith
- ‚ö†Ô∏è Peut √™tre limit√© par les moteurs de recherche (Google bloque souvent)
- ‚ö†Ô∏è N√©cessite plus de requ√™tes pour obtenir beaucoup de r√©sultats

#### Option B : M√©thode BuiltWith API (Payante)

Le script `discover.py` interroge l'API BuiltWith pour d√©couvrir des sites Shopify.

**Ex√©cution :**

```bash
# M√©thode gratuite (recommand√©e)
python discover_free.py

# OU m√©thode BuiltWith (payante)
python discover.py
```

**Fonctionnement de discover_free.py :**

Le script utilise Selenium (par d√©faut) pour simuler un navigateur r√©el et √©viter les blocages. Il parcourt exactement 100 pages pour chaque requ√™te de recherche, ce qui permet de d√©couvrir un grand nombre de sites Shopify.

Le script utilise plusieurs m√©thodes en parall√®le :

**Mode Catalog** (`BUILTWITH_API_METHOD=catalog`) - **RECOMMAND√â** :
- Acc√®de directement au catalogue BuiltWith des sites Shopify
- R√©cup√®re la liste compl√®te (ou pagin√©e) des sites Shopify
- M√©thode la plus efficace et rapide
- N√©cessite un plan payant avec acc√®s au catalogue
- Les domaines sont sauvegard√©s dans `domains_to_scrape.txt`
- Vous pouvez limiter le nombre de r√©sultats avec `MAX_RESULTS` dans `.env`

**Mode Technology Search** (`BUILTWITH_API_METHOD=technology_search`) :
- Utilise l'API Technology Search de BuiltWith
- Recherche des sites utilisant Shopify avec possibilit√© de filtres
- N√©cessite un plan payant avec acc√®s √† cette fonctionnalit√©
- Les domaines uniques sont sauvegard√©s dans `domains_to_scrape.txt`

**Mode Domain Lookup** (`BUILTWITH_API_METHOD=domain_lookup`) :
- Lit un fichier `domains_to_check.txt` (un domaine par ligne)
- V√©rifie chaque domaine pour confirmer l'utilisation de Shopify
- Les domaines confirm√©s sont sauvegard√©s dans `domains_to_scrape.txt`
- Utile si vous avez d√©j√† une liste de domaines √† v√©rifier

**Personnalisation :**

Vous pouvez modifier la liste de mots-cl√©s dans la fonction `main()` de `discover.py` :

```python
keywords = [
    "boutique de v√™tements",
    "jewelry store",
    "fashion france",
    "votre mot-cl√© ici"
]
```

**Gestion des erreurs :**

- Le script g√®re automatiquement les limites de taux de l'API (429)
- Les erreurs d'authentification sont signal√©es
- Un m√©canisme de retry est impl√©ment√© pour les erreurs temporaires

### Phase 2 : Scraping des pages de contact

Le script `scraper.py` lit le fichier `domains_to_scrape.txt`, visite chaque page de contact, extrait les informations et les stocke en base de donn√©es.

**Ex√©cution :**

```bash
python scraper.py
```

**Fonctionnement :**

1. Lit le fichier `domains_to_scrape.txt` ligne par ligne
2. Pour chaque domaine :
   - Cherche la page de contact (essaie `/pages/contact` puis `/contact`)
   - Scrape le contenu HTML de la page
   - Extrait l'adresse e-mail (premi√®re trouv√©e)
   - Extrait le num√©ro de t√©l√©phone (premier trouv√©, format international ou fran√ßais)
   - Sauvegarde les donn√©es en base de donn√©es PostgreSQL

**Priorit√© des URLs de contact :**

1. `https://[domaine]/pages/contact`
2. `https://[domaine]/contact`
3. `https://[domaine]/pages/contact-us`
4. `https://[domaine]/contact-us`

**Gestion des erreurs :**

- Retry automatique en cas d'erreur serveur (5xx)
- Gestion des timeouts
- Gestion des erreurs r√©seau
- Les domaines sans page de contact sont quand m√™me enregistr√©s (sans email/phone)

**Pr√©vention des doublons :**

Le script utilise `ON CONFLICT (url) DO NOTHING` pour √©viter les doublons si un domaine est trait√© plusieurs fois.

## üìä Base de Donn√©es

### Structure de la table `shopify_contacts`

| Colonne | Type | Description |
|---------|------|-------------|
| `id` | SERIAL | Identifiant unique (cl√© primaire) |
| `url` | VARCHAR(255) | URL du domaine Shopify (unique) |
| `email` | VARCHAR(255) | Adresse e-mail extraite |
| `phone_number` | VARCHAR(50) | Num√©ro de t√©l√©phone extrait |
| `contact_page_url` | VARCHAR(255) | URL de la page de contact utilis√©e |
| `scraped_at` | TIMESTAMP WITH TIME ZONE | Date et heure du scraping |

### Requ√™tes utiles

**Voir tous les contacts extraits :**

```sql
SELECT * FROM shopify_contacts ORDER BY scraped_at DESC;
```

**Compter les contacts avec email :**

```sql
SELECT COUNT(*) FROM shopify_contacts WHERE email IS NOT NULL;
```

**Compter les contacts avec t√©l√©phone :**

```sql
SELECT COUNT(*) FROM shopify_contacts WHERE phone_number IS NOT NULL;
```

**Voir les statistiques :**

```sql
SELECT 
    COUNT(*) as total,
    COUNT(email) as avec_email,
    COUNT(phone_number) as avec_telephone,
    COUNT(*) FILTER (WHERE email IS NOT NULL AND phone_number IS NOT NULL) as avec_les_deux
FROM shopify_contacts;
```

## üîß Configuration Avanc√©e

### Modifier les param√®tres de scraping

Dans `scraper.py`, vous pouvez ajuster :

- `MAX_RETRIES` : Nombre de tentatives en cas d'erreur (d√©faut: 3)
- `RETRY_DELAY` : D√©lai entre les tentatives en secondes (d√©faut: 2)
- `REQUEST_TIMEOUT` : Timeout des requ√™tes HTTP en secondes (d√©faut: 10)
- `USER_AGENT` : User-Agent utilis√© pour les requ√™tes

### Modifier les patterns d'extraction

Les expressions r√©guli√®res pour l'extraction d'email et de t√©l√©phone sont d√©finies dans `scraper.py` :

- `EMAIL_PATTERN` : Pattern pour les adresses e-mail
- `PHONE_PATTERN` : Pattern pour les num√©ros de t√©l√©phone (fran√ßais et international)

## ‚ö†Ô∏è Notes Importantes

1. **Respect des robots.txt** : Ce script ne v√©rifie pas automatiquement les fichiers robots.txt. Assurez-vous de respecter les conditions d'utilisation des sites web que vous scrapez.

2. **Limites de taux** : L'API BuiltWith a des limites de taux. Le script g√®re automatiquement les erreurs 429, mais vous devrez peut-√™tre ajuster les d√©lais entre les requ√™tes.

3. **Performance** : Le scraping peut √™tre lent pour un grand nombre de domaines. Le script inclut des pauses entre les requ√™tes pour √©viter la surcharge.

4. **Donn√©es extraites** : Les patterns d'extraction peuvent ne pas capturer tous les formats d'email/t√©l√©phone. Vous pouvez les ajuster selon vos besoins.

## üêõ D√©pannage

### Erreur de connexion √† la base de donn√©es

- V√©rifiez que PostgreSQL est d√©marr√©
- V√©rifiez les identifiants dans `.env`
- V√©rifiez que la base de donn√©es existe

### Erreur d'API BuiltWith

- V√©rifiez que votre cl√© API est correcte dans `.env`
- V√©rifiez que vous n'avez pas d√©pass√© les limites de votre plan API
- Attendez quelques minutes si vous recevez des erreurs 429 (limite de taux)

### Aucune donn√©e extraite

- V√©rifiez que les pages de contact contiennent bien des emails/t√©l√©phones en texte brut
- Certains sites utilisent des formulaires JavaScript qui ne sont pas accessibles via BeautifulSoup
- Les emails/t√©l√©phones dans des images ne seront pas d√©tect√©s

## üìù Licence

Ce projet est fourni tel quel, sans garantie. Utilisez-le de mani√®re responsable et respectez les conditions d'utilisation des sites web que vous scrapez.

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† ouvrir une issue ou une pull request.
