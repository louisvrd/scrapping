# Scraper Shopify - Architecture LÃ©gale

Outil de collecte d'URLs de boutiques Shopify utilisant uniquement des sources lÃ©gales et publiques.

## ğŸ¯ Objectifs

- **LÃ©galitÃ©** : Respect de robots.txt, conditions d'utilisation, rate limiting
- **Robustesse** : Gestion d'erreurs, retry, logging complet
- **ModularitÃ©** : Architecture claire avec scrapers sÃ©parÃ©s
- **Pagination** : Gestion automatique de la pagination
- **DÃ©tection Shopify** : Filtrage intelligent des URLs Shopify

## ğŸ“ Structure du Projet

```
.
â”œâ”€â”€ main.py                 # Script principal
â”œâ”€â”€ config.py              # Configuration centralisÃ©e
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ .env.example           # Exemple de configuration
â”œâ”€â”€ scrapers/              # Modules de scraping
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_scraper.py    # Classe de base pour tous les scrapers
â”‚   â”œâ”€â”€ certificate_transparency.py  # Scraper CT Logs
â”‚   â”œâ”€â”€ annuaire_scraper.py          # Scraper annuaires publics
â”‚   â””â”€â”€ custom_urls_scraper.py        # Scraper URLs personnalisÃ©es
â”œâ”€â”€ utils/                 # Utilitaires
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py          # Configuration du logging
â”‚   â”œâ”€â”€ robots_checker.py  # VÃ©rification robots.txt
â”‚   â””â”€â”€ shopify_detector.py # DÃ©tection Shopify
â”œâ”€â”€ output/                # Fichiers de sortie
â”‚   â”œâ”€â”€ shopify_urls.json
â”‚   â””â”€â”€ shopify_urls.csv
â””â”€â”€ logs/                  # Fichiers de log
    â””â”€â”€ scraper.log
```

## ğŸš€ Installation

1. **Installer les dÃ©pendances** :
```bash
pip install -r requirements.txt
```

2. **Configurer l'environnement** :
```bash
cp .env.example .env
# Ã‰diter .env avec vos paramÃ¨tres
```

## âš™ï¸ Configuration

### Sources de donnÃ©es

Le scraper supporte plusieurs sources lÃ©gales :

1. **Certificate Transparency Logs** (`crt.sh`)
   - API publique et lÃ©gale
   - Trouve tous les domaines `*.myshopify.com`
   - Aucune pagination nÃ©cessaire

2. **Annuaires publics**
   - `shop.app` (annuaire de sites Shopify) - **Note**: Peut bloquer les requÃªtes automatisÃ©es (403)
   - Gestion automatique de la pagination
   - **Recommandation**: DÃ©sactivÃ© par dÃ©faut car certains sites bloquent les scrapers

3. **URLs personnalisÃ©es**
   - Liste de pages Ã  scraper (fichier texte)
   - Une URL par ligne

### ParamÃ¨tres principaux

Dans `.env` :

```env
# Activer/dÃ©sactiver les sources
CT_LOGS_ENABLED=true
ANNUAIRES_ENABLED=true
SHOP_APP_ENABLED=false  # DÃ©sactivÃ© par dÃ©faut (bloque souvent avec 403)
CUSTOM_URLS_ENABLED=false

# DÃ©lais (secondes)
DELAY_BETWEEN_REQUESTS=2.0
DELAY_BETWEEN_PAGES=3.0

# Limites
MAX_PAGES_PER_SOURCE=100
MAX_RETRIES=3

# Respect robots.txt
RESPECT_ROBOTS_TXT=true

# Format de sortie
OUTPUT_FORMAT=json  # ou 'csv'
```

## ğŸ“– Utilisation

### Utilisation basique

```bash
python main.py
```

Le script va :
1. Scraper les sources activÃ©es
2. Extraire toutes les URLs de chaque page
3. Filtrer pour ne garder que les URLs Shopify
4. Sauvegarder les rÃ©sultats dans `output/shopify_urls.json` (ou `.csv`)

### Utiliser des URLs personnalisÃ©es

1. CrÃ©er un fichier `custom_urls.txt` :
```
https://example.com/shopify-stores
https://another-site.com/listings
```

2. Activer dans `.env` :
```env
CUSTOM_URLS_ENABLED=true
CUSTOM_URLS_FILE=custom_urls.txt
```

3. Lancer le script :
```bash
python main.py
```

## ğŸ” DÃ©tection Shopify

Le scraper dÃ©tecte les sites Shopify via :

1. **VÃ©rification rapide** (sans requÃªte HTTP) :
   - Domaines `*.myshopify.com`
   - Patterns dans l'URL (cdn.shopify.com, etc.)

2. **VÃ©rification approfondie** (optionnelle) :
   - Analyse du contenu HTML
   - DÃ©tection de patterns Shopify dans le code
   - ActivÃ©e avec `DEEP_VERIFICATION=true` dans `.env`

## ğŸ“Š Logs

Les logs sont sauvegardÃ©s dans :
- Console : niveau INFO
- Fichier `logs/scraper.log` : niveau DEBUG

Exemple de log :
```
2024-01-15 10:30:00 - shopify_scraper - INFO - DÃ©but du scraping Certificate Transparency
2024-01-15 10:30:05 - shopify_scraper - INFO - 1500 URLs Shopify trouvÃ©es
```

## ğŸ›¡ï¸ Respect de la lÃ©galitÃ©

Le scraper respecte :

- âœ… **robots.txt** : VÃ©rification automatique avant chaque requÃªte
- âœ… **Rate limiting** : DÃ©lais configurables entre requÃªtes
- âœ… **User-Agent** : Identification claire du bot
- âœ… **Conditions d'utilisation** : Utilisation uniquement de sources publiques
- âŒ **Pas de contournement** : Aucun bypass de captcha, paywall, ou anti-bot

## ğŸ”§ Personnalisation

### Ajouter une nouvelle source

1. CrÃ©er un nouveau scraper dans `scrapers/` :
```python
from scrapers.base_scraper import BaseScraper

class MyCustomScraper(BaseScraper):
    def get_next_page_url(self, current_url, html):
        # Logique de pagination
        pass
    
    def scrape(self, start_url):
        # Logique de scraping
        pass
```

2. L'ajouter dans `main.py` :
```python
from scrapers.my_custom_scraper import MyCustomScraper

scraper = MyCustomScraper()
urls = scraper.scrape("https://example.com")
```

### Modifier la dÃ©tection Shopify

Ã‰diter `config.py` pour ajouter/modifier les patterns :
```python
SHOPIFY_PATTERNS = [
    r'\.myshopify\.com',
    r'votre_pattern_ici',
]
```

## ğŸ“ Format de sortie

### JSON
```json
{
  "total_urls": 1500,
  "urls": [
    "https://store1.myshopify.com",
    "https://store2.myshopify.com",
    ...
  ]
}
```

### CSV
```csv
url
https://store1.myshopify.com
https://store2.myshopify.com
...
```

## âš ï¸ Limitations

- Les annuaires avec scroll infini nÃ©cessitent Selenium/Playwright (non implÃ©mentÃ© par dÃ©faut)
- La vÃ©rification approfondie (`DEEP_VERIFICATION`) augmente le temps de scraping
- Certains sites peuvent bloquer les requÃªtes automatisÃ©es (normal et lÃ©gal)

## ğŸ¤ Contribution

Pour ajouter une nouvelle source lÃ©gale :
1. CrÃ©er un nouveau scraper dans `scrapers/`
2. Respecter l'interface `BaseScraper`
3. Ajouter la configuration dans `config.py`
4. Documenter dans le README

## ğŸ“„ Licence

Ce projet est destinÃ© Ã  un usage lÃ©gal et Ã©thique. Respectez toujours les conditions d'utilisation des sites scrapÃ©s.

