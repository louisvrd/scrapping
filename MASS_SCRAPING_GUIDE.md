# Guide de Scraping Massif - 4 Millions de Sites Shopify

Ce guide explique comment utiliser le mode scraping massif pour d√©couvrir le maximum de sites Shopify.

## üöÄ Utilisation Rapide

### Scraping Massif Complet (Recommand√©)
```bash
python main.py --massive
```

Cette commande va :
- **Scraper shop.app** : Explorer toutes les cat√©gories, combinaisons, pages
- **Rechercher sur le web** : Utiliser Google, Bing, DuckDuckGo avec des milliers de requ√™tes
- **Combiner les r√©sultats** : shop.app + moteurs de recherche pour maximum de sites
- **Peut prendre plusieurs jours et g√©n√©rer des millions d'URLs**

### Strat√©gies Sp√©cifiques

**Seulement les cat√©gories** (plus rapide) :
```bash
python main.py --massive --strategy categories
```

**Seulement les combinaisons de recherche** :
```bash
python main.py --massive --strategy combinations
```

**Seulement la d√©couverte automatique** :
```bash
python main.py --massive --strategy discovery
```

**Seulement la pagination agressive** :
```bash
python main.py --massive --strategy pagination
```

## üìä Strat√©gies Disponibles

### 1. `comprehensive` (Par d√©faut)
- Combine toutes les strat√©gies
- Maximum de sites trouv√©s
- **Tr√®s long** (plusieurs jours)

### 2. `categories`
- Explore 50+ cat√©gories
- 1000 pages par cat√©gorie
- **Rapide et efficace**

### 3. `combinations`
- Recherches par lettres (a-z, aa-zz)
- Recherches par chiffres (0-99)
- Mots-cl√©s courants
- **Bon compromis**

### 4. `discovery`
- Exploration r√©cursive profonde (niveau 5)
- D√©couvre automatiquement toutes les pages
- **Moyennement rapide**

### 5. `pagination`
- Pagination agressive de la recherche g√©n√©rale
- Jusqu'√† 5000 pages
- **Simple mais efficace**

### 6. `web`
- Recherche uniquement sur les moteurs de recherche web
- Google, Bing, DuckDuckGo
- **Bon pour trouver des sites non list√©s sur shop.app**

## ‚öôÔ∏è Configuration

Dans `config.py`, vous pouvez ajuster :

```python
MASS_SCRAPE_MAX_PAGES = 1000  # Pages par recherche
MASS_SCRAPE_MAX_WORKERS = 10  # Threads parall√®les
MASS_SCRAPE_MAX_DEPTH = 5     # Profondeur d'exploration
DELAY_BETWEEN_REQUESTS = 1    # D√©lai entre requ√™tes (secondes)
```

## üíæ Espace Disque Requis

Pour 4 millions de sites :
- **JSON** : ~2-4 GB
- **CSV** : ~500 MB - 1 GB
- **Total recommand√©** : 10 GB d'espace libre

## ‚è±Ô∏è Temps Estim√©

- **Cat√©gories seulement** : 2-5 heures
- **Combinaisons** : 5-10 heures  
- **Comprehensive** : 2-7 jours (selon votre connexion)

## üîß Optimisations

### Pour aller plus vite :
1. R√©duire `DELAY_BETWEEN_REQUESTS` √† 0.5 secondes
2. Augmenter `MASS_SCRAPE_MAX_WORKERS` √† 20
3. D√©sactiver la v√©rification : ne pas utiliser `--verify-all`

### Pour plus de sites :
1. Augmenter `MASS_SCRAPE_MAX_PAGES` √† 5000
2. Augmenter `MASS_SCRAPE_MAX_DEPTH` √† 7
3. Utiliser `--strategy comprehensive`

## ‚ö†Ô∏è Avertissements

1. **Ressources** : Le scraping massif utilise beaucoup de CPU, RAM et bande passante
2. **Temps** : Peut prendre plusieurs jours pour 4 millions de sites
3. **Espace disque** : Assurez-vous d'avoir suffisamment d'espace
4. **Rate limiting** : shop.app peut limiter les requ√™tes si trop agressif

## üìù Exemple de Sortie

```
=== SCRAPING MASSIF DE SHOP.APP ===

=== STRAT√âGIE 1: EXPLORATION DES CAT√âGORIES ===
Exploration de 50 cat√©gories...
  Cat√©gorie 'fashion': 1250 URLs
  Cat√©gorie 'electronics': 890 URLs
  ...

=== STRAT√âGIE 2: RECHERCHES PAR COMBINAISONS ===
Scraping parall√®le de 1000 requ√™tes avec 10 workers...
  [1/1000] 'a' termin√©: 45 URLs
  ...

TOTAL FINAL: 3,847,293 URLs UNIQUES TROUV√âES
```

## üéØ Objectif : 4 Millions de Sites

Pour atteindre 4 millions de sites, utilisez :

```bash
# Commande optimale
python main.py --massive --strategy comprehensive
```

Et laissez tourner pendant plusieurs jours. Le script sauvegarde automatiquement les r√©sultats au fur et √† mesure.

